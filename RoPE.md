# 位置编码的必要性
如果不添加位置编码，注意力计算的时候，注意力权重不会随位置变化，不符合需求，需要的是注意力可以感知到位置的差异性。
# 绝对位置编码
## 训练式位置编码
初始化一个向量维度 * 向量长度的位置编码矩阵，与embedding相加。但是这种方式不具备长度外推性，因为过长的向量的位置编码没有经过训练。
## sinusoidal位置编码
transformer中的经典位置编码，公式为：
$$
\begin{cases} 
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10,000^{\frac{2i}{d_{model}}}}\right) \\
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10,000^{\frac{2i}{d_{model}}}}\right) 
\end{cases}
$$
pos：行索引，代表第几个位置（第几个词）
i:列索引，代表ebd内部的维度
**$i$ 决定了“尺子的刻度”**：
随着 $i$ 的增大，$10000^{\frac{2i}{d_{model}}}$ 这个分母会指数级变大。这意味着：
- 当 $i$ 很小时（低维），波长很短，函数值随 $pos$ 的变化非常剧烈（高频）。
- 当 $i$ 很大时（高维），波长很长，函数值随 $pos$ 的变化非常缓慢（低频）。
 **$pos$ 是在刻度上的“取值点”**：
它决定了在当前频率的波形上，我们走到了哪一步。

这种编码方式的特点包括：
- Sinusoidal 编码具有周期性，有一定外推性，但效果有限
- 远程衰减性质：相同的ebd，距离越近，内积越高，反之越低。
# RoPE编码
**RoPE位置编码通过将一个向量旋转某个角度，为其赋予位置信息**
## 动机
绝对位置编码的方式，无法显式感知两个变量之间的相对位置关系。希望能实现相对的位置编码。核心逻辑在于：输入绝对位置m，n，输出的点积结果仅依赖于相对位移m-n。这样，模型可以学习到距离感，而非记住句子中的死板位置。
## 动机的公式说明
假设函数 $f(q, m)$ 表示为词向量 $q$ 添加绝对位置信息 $m$，得到带位置信息的向量 $q_m$：

$$q_m = f(q, m)$$
RoPE 希望 Query 向量 $q_m$ 与 Key 向量 $k_n$ 之间的**点积**能够包含相对位置信息 $m - n$。
为了实现这一目标，我们需要找到一个函数 $f$，使得两个位置向量的点积可以表示为一个仅关于词向量 $q, k$ 及其相对距离 $m - n$ 的函数 $g$：
$$f(q, m) \cdot f(k, n) = g(q, k, m - n)$$
## 实现方式
### 第一步：将向量看作复数 (2D 视角)

RoPE 首先将 $d$ 维的词向量两两一组，看作是 $\frac{d}{2}$ 个二维平面。对于其中一个二维平面上的向量 $[x_1, x_2]$，我们可以将其表示为一个复数 $z = x_1 + ix_2$。

### 第二步：应用旋转矩阵

为了引入位置信息 $m$，RoPE 不像传统方法那样做“加法”，而是做“乘法”——即将向量旋转一个角度。旋转角度与位置 $m$ 成正比：

$$f(q, m) = q \cdot e^{im\theta}$$

在二维矩阵的形式下，这等同于乘以一个**旋转矩阵**：

$$\begin{pmatrix} q_m^{(1)} \\ q_m^{(2)} \end{pmatrix} = \begin{pmatrix} \cos m\theta & -\sin m\theta \\ \sin m\theta & \cos m\theta \end{pmatrix} \begin{pmatrix} q^{(1)} \\ q^{(2)} \end{pmatrix}$$

其中，$\theta$ 是一个预设的频率常数
$$\theta_j = 10000^{-\frac{2(j-1)}{d}}$$
# 这里的 j = 1 代表向量的前两个维度 (index 0, 1)
# 这里的 j = 2 代表向量的下两个维度 (index 2, 3)

### 第三步：为什么点积满足相对位置？

这是 RoPE 最优雅的地方。根据复数的性质，两个复数点积（内积）等于一个复数与另一个复数的共轭相乘的实部。

如果我们有两个带位置信息的向量 $q_m$ 和 $k_n$：

1. $q_m$ 旋转了 $m\theta$ 度。
    
2. $k_n$ 旋转了 $n\theta$ 度。
    
3. 它们的点积结果会包含一项 $\cos(m\theta - n\theta)$，即 $\cos((m-n)\theta)$。
    

**结论：** 最终的点积结果只取决于它们的相对距离 $m-n$，完美达成了图片中所述的建模目标。
### 第四步：高维空间的整体实现

对于 $d$ 维向量，我们将整个向量切分成 $\frac{d}{2}$ 个部分，每一部分都按照上述逻辑旋转不同的角度 $\theta_i$：


$$
\mathbf{R}_{\Theta, m}^d \mathbf{x} = 
\begin{pmatrix}
\cos m\theta_1 & -\sin m\theta_1 & 0 & 0 & \cdots \\
\sin m\theta_1 & \cos m\theta_1 & 0 & 0 & \cdots \\
0 & 0 & \cos m\theta_2 & -\sin m\theta_2 & \cdots \\
0 & 0 & \sin m\theta_2 & \cos m\theta_2 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ \vdots
\end{pmatrix}
$$


---

### 总结 RoPE 的优势

- **外推性**：由于它是旋转，即使遇到比训练时更长的序列，模型也能通过旋转角度的线性增加来处理。
    
- **衰减性**：随着相对距离 $m-n$ 变大，点积强度会有自然衰减的趋势（长程衰减），符合直觉。
![[Pasted image 20260215122826.png]]
### 补充

- **外推性的局限**：Sinusoidal 编码虽然具有周期性，但在处理超出训练长度的序列时，模型往往难以泛化，效果远逊于 RoPE。
- **加法 vs 乘法**：
    - Sinusoidal 是 **Additive (加法)**：$Embedding = Word + Pos$。
    - RoPE 是 **Multiplicative (乘法/旋转)**：$Embedding = Rotate(Word, pos)$。这种旋转操作天然保持了向量的模长，仅改变相位，对优化更加友好。
- **Base 放大效应**：在长文本扩展中（如 LongLoRA），将 `base` 从 10,000 提升到 1,000,000 会拉长每个维度的周期。虽然增加了容纳长度，但也让“空间分辨率”变低了，模型会变得“近视”，分不清邻近位置的微小差异。因为：`base` 变大意味着 $\theta$ 变小，旋转得更“慢”了。就像时钟的指针走得极慢，导致相距很远的 $m$ 和 $n$ 对应的角度差依然很小，点积的 $\cos(m-n)\theta$ 依然接近 1。这确实会破坏模型对距离的敏感度。