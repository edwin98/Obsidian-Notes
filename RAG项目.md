# 背景
无线豆包是 ICT AI 辅助工作的重点项目，对标秘塔搜索：一个入口、报告式回答、答案可信。
## 最终性能
TOP20 返回时，E2E 准确率 93%。
# 整体架构
整体分为知识召回、知识库构建、知识预处理三部分。
**知识召回：**
- 多路召回：改写为三个问题，基于三个问题检索
- 三级召回：初筛、二筛、精排
- 混合召回：BM25关键字检索+向量检索
**知识库构建：**
- 向量知识库
- BM25词频库：通过无线专有词表，保证独立语义的词语不被分开
**知识预处理：**
- 知识标签：1+X+Y架构设计，保证知识有序存储。**关键**
- 知识切片：不同知识使用不同的切片方式
- 知识清洗：不同知识来源及格式，使用不同的清洗方式
## 多路召回
### 目的
将用户 query 相关的全部知识尽可能找全。
### 主要内容
1. 将指代内容还原，如多轮问答中，“随机接入是什么”“它有什么关键技术” → “随机接入有什么关键技术”。
2. 将原始问题扩展为 1~3 个问题，以适配 RAG 检索。如“CA 是什么” → “载波聚合是什么”。
#### 实现流程：
Prompt + LLM。在用户输入基础上进行属性识别与提取，提供给 LLM 作为 context。使用 Qwen3-4B 进行 query 改写，模型选择权衡准确度与时延，当前可控制在 1s 内。
#### prompt 的设计方式
##### user prompt
- 目标
- 受众
- 规则
	- 多轮问题解析规则
		- 指代词推断
		- 主语切换但意图延续的场景合理改写
		- 时间描述的推断
		- 人称指代的替换（如用户信息）
		- 相关的示例
	- 术语和关键词提取
	- 问题改写
	- 相关性排序
	- 最终输出
##### system prompt
- 目标
- 输出格式（对于json的限制描述）
- 输出示例
## 三级召回
仅使用一个 ES 库，为保证检索稳定性，采用三层召回。
1. 第一层：BM25 + 384 维向量库检索，各 1500 条，共 3000 条（实验发现准确率 99% 以上）。输入无线统一知识库，输出 3000 篇文档。
2. 第二层：BM25 + 768 维向量库检索，按用户 SAP 调整权重配比。输入 3000 篇文档 + 员工画像，输出 80 篇文档。
3. 第三层：rerank 精排，模型选择 [gte-multilingual-reranker-0.3B](https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base)，输入 80 篇文档，输出 10 篇文档。
三级召回耗时 2s：第一层 0.5s，第二层 0.5s，第三层 1s。
>[!question] 为什么要用两个维度的向量库
## 混合召回
混合召回是三层召回中的第二层策略：向量匹配侧重语义，BM25 基于关键字匹配。
### 具体实现
RSF 算法实现（Rank Score Fusion）：
1. 向量召回 160 篇文档，BM25 召回 160 篇文档
2. 向量召回、BM25 召回的得分归一化
3. 将二者加权求和，得到总得分
4. 对总得分排序，返回前80文档
权重$\alpha$的设置：根据用户tokens动态设置，问题短则偏BM25，问题长则偏向量，公式如下：
$$\alpha = 0.4 + \frac{0.3}{1 + e^{-\frac{L - k}{s}}}$$
- **$L$**：用户查询的 token 数量。
- **$k$**：中心值，设置为8，当长度等于 $k$ 时，$\alpha = 0.55$（正好是 $0.4$ 与 $0.7$ 的中点）。
- **$s$**：平滑系数，设置为1，控制从 $0.4$ 切换到 $0.7$ 的坡度。

## 知识库构建
### 向量知识库
embedding 模型选择：[gte-multilingual-base](https://huggingface.co/Alibaba-NLP/gte-multilingual-base)，基于该模型进行微调。微调重点在于高质量语料构建。
构建方式：基于 Qwen2.5 72B 自动生成语料对。
基于不同的数据内容，利用 prompt 指示大模型生成 JSON 格式的 QA 对：共生成 75w QA 对，数据处理后 48w 参与微调，在 A30 × 2 上训练两周。
要点：
- 保证问题的丰富性：分别从文档块、文档块标题、文档块生成摘要；摘要可作为问题，也可直接从文档块生成问题与答案。
- 后处理的必要性：异常字符剔除、minhash 问题去重、rerank 筛选、去除指代性问题（包含“本文”“这”“那”等字符）。
- 长度配比要搭配，使用不同长度的语料进行训练
- 同时微调多个维度
微调效果：在自有数据集上，top10:62->88；开源数据集（[DuRetriveal](https://huggingface.co/datasets/mteb/DuRetrieval)）上 98.8->97.8
### BM25词频库
有两个要点：
1. 统计知识量大，因此只建立一个知识库
2. TFIDF的正确计算，这要求文本能够正确分词，因此要构建无线词表
#### 词表的构建
1. 将DTS单、内容；support网站上所有支持文档、所有IDP文档均作为语料库内容
2. 清洗语料库，去重、去除目录、去除特殊符号、去除html等
3. 爬取华为术语的内容，作为最基础的正确标签，约5000词；维基百科高频词作为通用领域的负样本
4. 使用AutoPhrase算法，发现新词，强制保留“名+名”或“形+名”结构，大幅过滤无意义的动词性短语
5. 人工校正，将正确词作为标签数据，重复进行AtuoPhrase迭代，最终得到10000词，人工校正时优先挑选算法评分在 0.5 左右的边界样本，最大化迭代增益。
6. 使用Qwen2.5-72B进行打分过滤，结合人工审核，最终得到6800有效词
## 知识预处理
要点：所有的文本内容全部转成md格式；md格式切片主要目的为保留语义
### 知识格式处理
不同的格式的处理方式：
- word：转md
- html：转md
- pptx：按页处理，转成图片，ocr提取文字，生成图片链接
- excel/csv:转文本
- pdf：转md
- 图片：保留链接，链接不参与召回，仅参与总结
不同数据源的针对性处理：
- support文档：增加目录结构信息、特定内容增加标签（gnodeB增加5G标签）
- wiki：增加目录层级
- icase：这是一些反思案例，内容不做切片处理，标题增加产品信息
- 设计文档：仅取最新文档，增加PDU信息
- 3GPP：缩进做适配；文件名和协议编号重映射
- 华为知道、华为案例：不切片
### 知识切片
按照章节切分：
- 非叶子节点：包括子章节的文本段；若大于2K token，则生成总结作为替换chunk，同时继续切割，否则直接作为chunk
- 叶子节点：
- 无标题节点

| 维度            | 非叶子节点 (章节层级)                  | 叶子节点 (最小语义单元)     | 无标题 / 特殊节点 (正文块)           |
| ------------- | ----------------------------- | ----------------- | -------------------------- |
| 判定标准          | 带有子标题的导航层级（如 `#` / `##`）      | 文档末端的纯文本 / 细节描述段落 | 连续长文本、代码块、或数学公式组           |
| 切分规模          | 阈值：2K Tokens                  | 512 ~ 800 Tokens  | 继承父级大小，按语义边界对齐             |
| 核心动作          | ≤2K：保留全文并打标；>2K：LLM 生成摘要作为替换块 | 固定步长 + 语义换行切割     | 强制关联：继承最近一个有效标题            |
| 重叠度 (Overlap) | N/A（由摘要覆盖全局语义）                | 10% ~ 15%         | 保持上下文连贯，避免截断公式 / 代码        |
| 元数据注入         | 包含子节点 ID 列表、层级路径              | 回填父级摘要、层级路径、原始页码  | 标注 `is_continuation: true` |
| 检索价值          | 解决“宏观 / 综述类”问题召回              | 解决“具体事实 / 数值”问题召回 | 提供技术实现细节、代码片段              |


全流程的过程
补充框架图  [[ToDo]]
设计原则
哪些同步、哪些异步
性能瓶颈点
# 基层模型的选择
选择的因素
# chunk方式
chunk的单位
不同文档的chunk策略

# prompt设计
# 权限设计
检索过滤方式
embedding和权限控制的顺序
# 性能和成本
查询耗时及分布（embedding、rerank、llm）
# 评估方式
指标
离线评估、在线评估？
如何区分不同失败场景？检索、rerank、幻觉
# 数据预处理
数据清洗
结构化抽取
表格、图片处理方式
## 词表构建方式
AutoPxx
# 上下文处理方式
记忆处理方式
# 索引方式
向量库的选择
索引更新
# 重排方式
重排收益、延迟
# 失败处理方式
定义
兜底策略
防止hallucination prompt设计
# 使用人次及评价
迭代
迭代频率
# 框架选择
选择原因
自己封装的组件
遇到过的坑
# 压力测试
# 代码实现细节
## 前端的框架
## 后段框架及技术栈
## 前后端是怎么交互的
## 非阻塞时响应怎么做的
## 对话记忆的底层实现

# 多模态的处理