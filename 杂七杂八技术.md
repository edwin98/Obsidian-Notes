# MCP vs Agent Skill

> 核心区别：**MCP 是能力接入协议与运行时（runtime），Agent Skill 是 Agent 内部可调用的能力单元**

---

## 一句话理解

- **MCP（Model Context Protocol）**  
  统一、标准化地把**外部能力**暴露给 LLM / Agent

- **Agent Skill**  
  Agent 内部封装好的、可被规划器调用的**具体动作**

---

## 核心差异对比

| 维度 | MCP | Agent Skill |
|---|---|---|
| 抽象层级 | 系统级 / 平台级 | Agent 级 |
| 本质 | 协议 + 服务 | 函数 / Tool |
| 是否独立进程 | 是 | 否 |
| 是否可复用 | 跨 Agent | 通常仅单 Agent |
| 是否绑定 Agent | 否 | 是 |

---

## 职责边界

### MCP 做什么

- 能力注册与发现（Schema）
- 统一调用协议
- 权限 / 安全 / 隔离
- 连接真实系统（DB / 文件 / IDE / 仿真器）

### Agent Skill 做什么

- 封装业务动作
- 参数校验与逻辑
- 调用 MCP / API / 本地函数
- 给 Planner 返回可用结果

**都属于 LLM 推理控制 / 思维组织策略（Reasoning Strategy）**

---

# CoT、ToT、GoT

## CoT (Chain of Thought)

引导模型将思维过程输出，这样能提高模型的表现。CoT 有两种实现方式：

- **Zero-shot**：直接在 Prompt 后增加一句话，如 "Let's think step by step"。
- **Few-shot**：在 Prompt 中给模型几个带有推理步骤的例子。

### 应用场景

CoT 最适合那些**路径唯一但步骤繁琐**的任务。它解决了模型“跳步”导致的计算错误或逻辑断裂。

- **数学与科学计算**：
  在解决初高中数学应用题（如 GSM8K 数据集）时，CoT 能防止模型直接给出一个错误的数字。它会先列方程，再逐步求解。
- **代码逻辑解释**：
  当你给模型一段复杂的 Python 代码并询问其运行结果时，CoT 会让模型模拟解释器的行为，一行行追踪变量的变化，而不是凭感觉猜测输出。
- **简单常识推理**：
  例如判断“金字塔能否装进行李箱”，模型需要通过 CoT 先检索金字塔的大小，再检索行李箱的大小，最后进行对比。

### 具体的实现方式

对于大部分模型，属于提示词策略；**一部分模型使用强化学习（RL）将这个思考过程内化在权重中**，成为了模型的原生能力，如 DeepSeek-R1、OpenAI o1 等。

### CoT 为什么能降低幻觉

1. **概率空间收敛**：本质上大模型是尝试预测下一个 Token 的概率分布。在 CoT 中，相当于引入了中间变量，每步推理都能为下一步 Token 预测提供更强的上下文约束。
2. **激活模型的慢思考模式**：模型有更多计算资源用于推理。
3. **强化注意力锚定**：在 CoT 的过程中，自注意力会更关注自己推理出的一些步骤信息，避免关键信息丢失。同时，如果 CoT 中有矛盾出现，也会有自我纠错的能力。
4. **从检索模式切换为推理模式**。

### 局限

虽然 CoT 能降低幻觉，但它不能完全消除幻觉。如果：
1. **起始步骤错误**：第一步就错了，后面会引发“幻觉级联”（Hallucination Cascade），错得更离谱。
2. **知识盲区**：模型根本不知道某个核心事实，它会一本正经地进行错误的推导。

这就是为什么现在的趋势是 **RAG（检索增强生成）+ CoT**：用 RAG 保证事实节点是正确的，用 CoT 保证逻辑链条是严密的。

---

## ToT (Tree of Thought)

CoT 为线性，一步有误，则后续都会有问题，ToT 就是为了解决这个问题。这是 **Agent 的设计范式**。

### 工作流程

- **思维分解**：将问题分解为多个中间步骤（Thought Units）。
- **生成候选**：在每一步，模型会生成多个可能的下一步方案（分叉）。
- **状态评估**：由模型充当“评价者”，判断当前的各个分支哪个更有前途（可行、可能、不可能）。
- **搜索算法**：利用 DFS、BFS 来寻找最优解。如果发现当前路径死路一条，模型可以回溯到上一个节点重新选择。

### 应用场景

ToT 适用于**有多个潜在解、需要反复尝试和评估**的任务。它本质上是给 LLM 增加了“后悔药”和“全局观”。

- **创意写作与剧本构思**：
  在写小说时，作者可以利用 ToT 让模型生成三个不同的情节走向（分支 A、B、C），然后评估哪个情节逻辑最通顺、最吸引人，选定后再继续向下延伸。
- **复杂的任务规划**：
  比如“策划一场去西藏的旅游”。ToT 可以同时考虑“自驾”、“飞行”和“火车”三条树状路径，并根据预算和时间节点剔除不合理的路径。

---

## GoT (Graph of Thoughts) - 思维图

核心逻辑：**非线性网络与知识聚合**

GoT 是对 ToT 的进一步升华，它认为人类的思维不只是分叉的树，还可以是错综复杂的有向图（DAG）。这是 **Agent 的设计范式**。

### 工作方式

- **节点与边**：每个思维是一个节点，边表示推理关系。
- **变换操作**：
    - **聚合（Aggregation）**：将多个不同的思维节点合并成一个更完善的结论。
    - **精炼（Refinement）**：对某个思维节点进行迭代修正。
    - **循环（Looping）**：虽然通常是无环图，但在某些设计中允许对同一思维进行循环打磨。

**核心优势**：它可以处理具有复杂依赖关系的问题，支持不同推理路径之间的信息交换和优势互补。

### 应用场景

- **长文档摘要与信息整合**：如果要总结一本 300 页的书，GoT 可以先分章节总结（生成节点），然后将相关的章节总结进行合并（Aggregation），最后通过对比和去重，精炼出（Refinement）全书的核心观点。
- **复杂系统架构设计**：在设计一个微服务架构时，模型可以先生成存储方案和通信方案，然后将这两个节点连接起来，观察它们是否兼容。如果不兼容，则返回修改其中一个节点，直到整张图逻辑自洽。
- **新药发现或材料科学**：研究人员可以利用 GoT 让模型组合不同的化学分子式。节点 A 是某种特性，节点 B 是另一种特性，通过图的合并操作，探索是否存在某种新的组合能同时满足两者的优势。

---

# 位置编码

## 位置编码的必要性

如果不添加位置编码，注意力计算的时候，注意力权重不会随位置变化，不符合需求。我们需要的是注意力可以感知到位置的差异性。

---

## 绝对位置编码

### 训练式位置编码

初始化一个 `向量维度 * 向量长度` 的位置编码矩阵，与 Embedding 相加。但是这种方式不具备长度外推性，因为过长的向量的位置编码没有经过训练。

### Sinusoidal 位置编码

Transformer 中的经典位置编码，公式为：

$$
\begin{cases} 
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right) \\
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{model}}}}\right) 
\end{cases}
$$

- $pos$：行索引，代表第几个位置（第几个词）
- $i$：列索引，代表 embedding 内部的维度

**$i$ 决定了“尺子的刻度”**：
随着 $i$ 的增大，$10000^{\frac{2i}{d_{model}}}$ 这个分母会指数级变大。这意味着：
- 当 $i$ 很小时（低维），波长很短，函数值随 $pos$ 的变化非常剧烈（高频）。
- 当 $i$ 很大时（高维），波长很长，函数值随 $pos$ 的变化非常缓慢（低频）。

**$pos$ 是在刻度上的“取值点”**：
它决定了在当前频率的波形上，我们走到了哪一步。

**这种编码方式的特点包括：**
- Sinusoidal 编码具有周期性，有一定外推性，但效果有限。
- 远程衰减性质：相同的 embedding，距离越近，内积越高，反之越低。

---

## RoPE 旋转位置编码

**RoPE（Rotary Position Embedding）位置编码通过将一个向量旋转某个角度，为其赋予位置信息。**

### 动机

绝对位置编码的方式，无法显式感知两个变量之间的相对位置关系。我们希望能实现相对的位置编码。

核心逻辑在于：输入绝对位置 $m$、$n$，输出的点积结果仅依赖于相对位移 $m-n$。这样，模型可以学习到距离感，而非记住句子中的死板位置。

### 动机的公式说明

假设函数 $f(q, m)$ 表示为词向量 $q$ 添加绝对位置信息 $m$，得到带位置信息的向量 $q_m$：

$$
q_m = f(q, m)
$$

RoPE 希望 Query 向量 $q_m$ 与 Key 向量 $k_n$ 之间的**点积**能够包含相对位置信息 $m - n$。

为了实现这一目标，我们需要找到一个函数 $f$，使得两个位置向量的点积可以表示为一个仅关于词向量 $q, k$ 及其相对距离 $m - n$ 的函数 $g$：

$$
f(q, m) \cdot f(k, n) = g(q, k, m - n)
$$

### 实现方式

#### 第一步：将向量看作复数 (2D 视角)

RoPE 首先将 $d$ 维的词向量两两一组，看作是 $\frac{d}{2}$ 个二维平面。对于其中一个二维平面上的向量 $[x_1, x_2]$，我们可以将其表示为一个复数 $z = x_1 + ix_2$。

#### 第二步：应用旋转矩阵

为了引入位置信息 $m$，RoPE 不像传统方法那样做“加法”，而是做“乘法”——即将向量旋转一个角度。旋转角度与位置 $m$ 成正比：

$$
f(q, m) = q \cdot e^{im\theta}
$$

在二维矩阵的形式下，这等同于乘以一个**旋转矩阵**：

$$
\begin{pmatrix} q_m^{(1)} \\ q_m^{(2)} \end{pmatrix} = \begin{pmatrix} \cos m\theta & -\sin m\theta \\ \sin m\theta & \cos m\theta \end{pmatrix} \begin{pmatrix} q^{(1)} \\ q^{(2)} \end{pmatrix}
$$

其中，$\theta$ 是一个预设的频率常数：

$$
\theta_j = 10000^{-\frac{2(j-1)}{d}}
$$

> **说明**：
> 这里 $j = 1$ 代表向量的前两个维度 (index 0, 1)；
> 这里 $j = 2$ 代表向量的下两个维度 (index 2, 3)。

#### 第三步：为什么点积满足相对位置？

这是 RoPE 最优雅的地方。根据复数的性质，两个复数点积（内积）等于一个复数与另一个复数的共轭相乘的实部。

如果我们有两个带位置信息的向量 $q_m$ 和 $k_n$：
1. $q_m$ 旋转了 $m\theta$ 度。
2. $k_n$ 旋转了 $n\theta$ 度。
3. 它们的点积结果会包含一项 $\cos(m\theta - n\theta)$，即 $\cos((m-n)\theta)$。

**结论**：最终的点积结果只取决于它们的相对距离 $m-n$，完美达成了上述的建模目标。

#### 第四步：高维空间的整体实现

对于 $d$ 维向量，我们将整个向量切分成 $\frac{d}{2}$ 个部分，每一部分都按照上述逻辑旋转不同的角度 $\theta_i$：

$$
\mathbf{R}_{\Theta, m}^d \mathbf{x} = 
\begin{pmatrix}
\cos m\theta_1 & -\sin m\theta_1 & 0 & 0 & \cdots \\
\sin m\theta_1 & \cos m\theta_1 & 0 & 0 & \cdots \\
0 & 0 & \cos m\theta_2 & -\sin m\theta_2 & \cdots \\
0 & 0 & \sin m\theta_2 & \cos m\theta_2 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ \vdots
\end{pmatrix}
$$

---

### 总结 RoPE 的优势

- **外推性**：由于它是旋转，即使遇到比训练时更长的序列，模型也能通过旋转角度的线性增加来处理。
- **衰减性**：随着相对距离 $m-n$ 变大，点积强度会有自然衰减的趋势（长程衰减），符合直觉。

![[Pasted image 20260215122826.png]]

### 补充

- **外推性的局限**：Sinusoidal 编码虽然具有周期性，但在处理超出训练长度的序列时，模型往往难以泛化，效果远逊于 RoPE。
- **加法 vs 乘法**：
    - Sinusoidal 是 **Additive (加法)**：$Embedding = Word + Position$。
    - RoPE 是 **Multiplicative (乘法 / 旋转)**：$Embedding = Rotate(Word, pos)$。这种旋转操作天然保持了向量的模长，仅改变相位，对优化更加友好。
- **Base 放大效应**：在长文本扩展中（如 LongLoRA），将 `base` 从 10,000 提升到 1,000,000 会拉长每个维度的周期。虽然增加了容纳长度，但也让“空间分辨率”变低了，模型会变得“近视”，分不清邻近位置的微小差异。因为：`base` 变大意味着 $\theta$ 变小，旋转得更“慢”了。就像时钟的指针走得极慢，导致相距很远的 $m$ 和 $n$ 对应的角度差依然很小，点积的 $\cos((m-n)\theta)$ 依然接近 1。这确实会破坏模型对距离的敏感度。
---
# 多模态的实现框架

大模型理解图片的底层原理，本质上是将“非结构化的图像信号”编码成“结构化的数学特征”，经过特征对齐后，再送入大语言模型（LLM）进行理解与生成。整个体系可从**系统流程**、**训练范式**、**核心组件**三个维度来拆解。

---

## 一、 系统架构与推理过程

多模态大模型的推理流水线主要分为三大步：**感知 (Perception)**、**对齐 (Alignment)** 和 **融合 (Fusion)**。

### 1. 视觉感知 (Perception)
主要通过 ViT (Vision Transformer) 或 CLIP 等视觉编码器提取图像特征。
- **图像切片 (Patching)**：将原始图像切割成网格状的固定大小方块（Patch）。
- **线性投影 (Linear Projection)**：将每个小图块展平，并映射为高维的 Embedding 向量。
- **位置编码 (Positional Encoding)**：为每个无序的方块打上绝对或相对位置的标签，保留图像的空间几何结构。
- **特征提取 (Feature Extraction)**：通过多层 Self-Attention 结构，让各个 Patch 相互交换信息，最终输出具备高阶语义的图像特征矩阵。

### 2. 语义对齐 (Alignment)
视觉编码器输出的高维向量与 LLM 原生的文本向量空间是不互通的，需要通过**适配器（Connector / Projector）**进行“翻译”和空间对齐。常见对齐方案：

| 方案 | 做法 | 特点 | 代表模型 |
| :--- | :--- | :--- | :--- |
| **线性网络 (Linear/MLP)** | 用线性层或多层感知机直接把视觉特征映射到文本维度。 | 简单粗暴，不损失视觉细节，但如果图像分辨率高，会有巨大的 Token 负担。 | LLaVA 系列 |
| **Q-Former (查询变换器)** | 使用一组设定好的“可学习 Query”去视觉特征里“交叉提取”信息。 | 压缩率极高，能过滤无关背景，降低 LLM 运算量。 | BLIP-2, InstructBLIP |
| **感知重采样 (Perceiver)** | 将任意长度的视觉特征固定压缩为几十个 Token。 | 适合处理极高分辨率图像或超长视频帧。 | Flamingo, IDEFICS |

### 3. 模态融合 (Fusion)
对齐后的图像特征已经被翻译成了 LLM 能听懂的“视觉 Token”。系统会直接将其与用户的“文本 Token”序列进行拼接（Concatenation）：

`[视觉 Token 1, 视觉 Token 2, ...] + [文本 Token: "请问图中有什么？"]`

最终统一送入 LLM 进行自回归推理。

---

## 二、 经典两阶段训练范式

要让独立的视觉模块和语言模型配合默契，通常需要分步骤训练。多模态模型的训练往往建立在强大的预训练 LLM 和预训练视觉编码器（如 CLIP ViT）之上。

### 阶段一：视觉-语言特征对齐 (Alignment Pre-training)
- **核心目标**：让模型“认识物体”，建立视觉概念与文本概念的强绑定。
- **训练数据**：海量且简单的“图像 - 较短描述”对（Image-Caption Pairs，例如数亿级别的 LAION 数据集：一张橘猫照片 + "a ginger cat"）。
- **模型状态**：通常**冻结（Freeze）** LLM 本身和视觉编码器的权重，仅仅训练中间的**对齐层（Connector）**。这一步是让对齐层变成一名“翻译官”，学会把视觉特征准确翻译成 LLM 理解的词汇空间。

### 阶段二：多模态指令微调 (Visual Instruction Tuning)
- **核心目标**：让模型具备复杂的视觉推理、视觉问答和遵守人类指令的能力。
- **训练数据**：经过专门构造的复杂图文对话数据（如：“这张图里左边的人在干什么？”、“这盘菜的菜谱是什么？”）。这些数据往往由 GPT-4V 或人工进行高质量构造。
- **模型状态**：保持视觉编码器冻结，**解冻对齐层并结合 LLM 的全量微调（Full Fine-tuning）或高效微调（如 LoRA）**，让 LLM 适应多模态 Token 的输入逻辑，从而学会根据图像内容进行逻辑判断与详细输出。

---

## 三、 核心组件底层细节

### 1. ViT (Vision Transformer)：视觉特征的提炼工厂
ViT 彻底改变了 CV 领域，证明了只要数据规模够大，Attention 机制可以有效替代卷积网络 (CNN)。

**底层核心机制：**
- **序列化 (Patch Embedding)**：假设输入图像尺寸为 $H \times W \times C$。ViT 将图像划分为 $N$ 个固定大小的 Patch（如 $16 \times 16$），则有：
  
  $$N = \frac{H \times W}{P^2}$$
  
  每个 Patch 展平为一个 $P^2 \cdot C$ 维的向量，经过线性层 $E$ 映射到 $D$ 维，并加入可训练的位置编码矩阵 $E_{pos}$：
  
  $$z_0 = [x^1_p E; x^2_p E; \dots; x^N_p E] + E_{pos}$$
  
  *(注：Transformer 注意力机制天然是输入置换不变的，没有 $E_{pos}$，模型就完全无法分辨物体在图里的空间相对位置。)*

- **注意力汇聚 (Self-Attention)**：通过注意力公式计算各 Patch 间的上下文相关性：
  
  $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
  
  这让模型理解“左上角的猫耳朵”和“右下角的猫尾巴”同属于一只猫。

- **特征输出策略**：
  - **CLIP 模式**：由于原本受限于 Image-Text 任务更关注全局的高层语义，通常只提取头部的 `[CLS]` Token 被用作图像的整图特征。
  - **LLaVA 模式**：直接摒弃 `[CLS]`，提取出所有的 $N$ 个 Patch 的空间特征矩阵（Grid Features），最大程度保留细粒度局部特征。

---

## 2. 线性层 / MLP (Linear Projector)：跨空间的虫洞
在 LLaVA 等基于投影对齐的架构中，这里承担桥梁重任，负责将 Visual Space 平滑映射到 LLM Space。

**底层核心机制：**
- **数学本质**：若视觉特征 $V \in \mathbb{R}^{N \times D_{vis}}$，LLM 自身输入维度为 $D_{llm}$，映射就是一个带有偏差权重的矩阵乘法 $W \in \mathbb{R}^{D_{vis} \times D_{llm}}$：
  
  $$X_{visual\_tokens} = V \cdot W + b$$

- **形态进化**：
  - **单层 Linear**：早期的 LLaVA-1.0 只做纯线性变换，表达能力有限。
  - **两层 MLP (多层感知机)**：LLaVA-1.5 及之后的模型全面跨入 `Linear -> GELU激活层 -> Linear` 的非线性结构。因为视觉和语言所处的空间往往极度扭曲，非线性能够赋予投影网络更广的“折叠对齐”空间，表达能力指数倍增。
  
- **局限性**：这是典型的“搬运工”模式。提取了 576 个 Token 就会原封不动作 576 次投影送入模型；面临 2K/4K 高清原图时，Token 爆炸式增长会引发显存告急和计算低效。

---

## 3. Q-Former (Query Transformer)：精密的语义榨汁机
BLIP 家族（BLIP-2 / InstructBLIP 等）的核心法宝。它是一个有“信息提纯认知”的轻量化中间引擎。

**底层核心机制：**
Q-Former 本质是一组包含双向与跨模态注意力的解耦 Transformer，其精髓在于内部预设了固定数量（如 32 个）的 **可学习注意力查询 (Learned Queries)**。

- **工作流与双重提取机制**：
  1. **Self-Attention 自注意力**：32 个 Query 相互交流，仿佛探针组内部正在“切分任务区域，避免重复抓取”。
  2. **Cross-Attention 交叉注意力 (精华所在)**：
     - **Query (Q)**: 32 个探针向量
     - **Key (K) / Value (V)**: ViT 输出的大量图像 Patch 实体特征
     
     $$Output = \text{Softmax}\left(\frac{Q_{query} \cdot K_{visual}^T}{\sqrt{d}}\right) V_{visual}$$
     
     这步相当于向极其庞大的原图特征中“定向提问”，强力吸附跟语义有关的“重要元素（前景实体、动作文本等）”。

- **Query 是怎么“诞生”的？**
  初始化时这 32 个查询向量只是随机的数（“乱码”），但在预训练阶段的海量图文对比学习下，通过重构与匹配损失函数的反向传播，它们被自动“塑形”。部分 Query 变成专门负责找“颜色”的专家，部分成了负责找“人脸”的专家。

- **核心势能：降本增效**：
  - **极端压缩（固定成本）**：图再大、ViT 算出来的 Token 数量再多，经过网络后被死死卡住在输出 32 个 Token；送给 LLM 端极度省流节流省算力。
  - **自适应去噪**：相比于全景扫描，这 32 把刷子天然懂得规避“大面积不含有意义信息的天空、纯色背景垫”，实现了硬核的特征提取提炼。