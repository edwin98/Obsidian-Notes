
# MCP vs Agent Skill

> 核心区别：**MCP 是能力接入协议与运行时（runtime），Agent Skill 是 Agent 内部可调用的能力单元**

---

## 一句话理解

- **MCP（Model Context Protocol）**  
  统一、标准化地把**外部能力**暴露给 LLM / Agent

- **Agent Skill**  
  Agent 内部封装好的、可被规划器调用的**具体动作**

---

## 核心差异对比

| 维度 | MCP | Agent Skill |
|---|---|---|
| 抽象层级 | 系统级 / 平台级 | Agent 级 |
| 本质 | 协议 + 服务 | 函数 / Tool |
| 是否独立进程 | 是 | 否 |
| 是否可复用 | 跨 Agent | 通常仅单 Agent |
| 是否绑定 Agent | 否 | 是 |

---

## 职责边界

### MCP 做什么
- 能力注册与发现（schema）
- 统一调用协议
- 权限 / 安全 / 隔离
- 连接真实系统（DB / 文件 / IDE / 仿真器）

### Agent Skill 做什么
- 封装业务动作
- 参数校验与逻辑
- 调用 MCP / API / 本地函数
- 给 Planner 返回可用结果

**都属于LLM 推理控制 / 思维组织策略（Reasoning Strategy）**

---

# CoT、ToT、GoT

# CoT(Chain of Thought)

引导模型将思维过程输出，这样能提高模型的表现， CoT有两种实现方式:
- zero shot:直接在prompt后增加一句话，Let's think step by step
- few shot:在 Prompt 中给模型几个带有推理步骤的例子

## 应用场景
CoT 最适合那些**路径唯一但步骤繁琐**的任务。它解决了模型“跳步”导致的计算错误或逻辑断裂。
- 数学与科学计算：
    在解决初高中数学应用题（如 GSM8K 数据集）时，CoT 能防止模型直接给出一个错误的数字。它会先列方程，再逐步求解。
- 代码逻辑解释：
    当你给模型一段复杂的 Python 代码并询问其运行结果时，CoT 会让模型模拟解释器的行为，一行行追踪变量的变化，而不是凭感觉猜测输出。
- 简单常识推理：
    例如判断“金字塔能否装进行李箱”，模型需要通过 CoT 先检索金字塔的大小，再检索行李箱的大小，最后进行对比。

## 具体的实现方式
对于大部分模型，属于提示词策略，**一部分模型使用RL将这个思考过程内化在权重中**，成为了模型的原生能力，如deepseek-R1、openAI-O1等
## CoT为什么能降低幻觉
1. 概率空间收敛：本质上大模型是长丝预测下一个token的概率分布，在cot中，相当于引入了中间变量，每步推理都能为下一步token预测提供更强的上下文约束
2. 激活模型的慢思考模式
3. 强化注意力锚定，在cot的过程中，自注意力会更关注自己推理出的一些步骤信息，避免关键信息丢失。同时，如果cot中有矛盾出现，也会有自我纠错的能力
4. 从检索模式切换为了推理模式
## 局限
虽然 CoT 能降低幻觉，但它不能完全消除幻觉。如果
1. **起始步骤错误**：第一步就错了，后面会引发“幻觉级联”（Hallucination Cascade），错得更离谱。
2. **知识盲区**：模型根本不知道某个核心事实，它会一本正经地进行错误的推导。
这就是为什么现在的趋势是 **RAG（检索增强生成）+ CoT**：用 RAG 保证事实节点是正确的，用 CoT 保证逻辑链条是严密的。
# ToT(Tree of Thought)
CoT为线性，一步有误，则后续都会有问题，ToT就是为了解决这个问题。是 **Agent 的设计范式**
## 工作流程
- 思维分解：将问题分解为多个中间步骤（Thought Units）。
- 生成候选：在每一步，模型会生成多个可能的下一步方案（分叉）。
- 状态评估：由模型充当“评价者”，判断当前的各个分支哪个更有前途（可行、可能、不可能）。
- 搜索算法：利用DFS、BFS来寻找最优解。如果发现当前路径死路一条，模型可以回溯到上一个节点重新选择。
## 应用场景
ToT 适用于**有多个潜在解、需要反复尝试和评估**的任务。它本质上是给 LLM 增加了“后悔药”和“全局观”。
- 创意写作与剧本构思：
    在写小说时，作者可以利用 ToT 让模型生成三个不同的情节走向（分支 A、B、C），然后评估哪个情节逻辑最通顺、最吸引人，选定后再继续向下延伸。
- 复杂的任务规划：
    比如“策划一场去西藏的旅游”。ToT 可以同时考虑“自驾”、“飞行”和“火车”三条树状路径，并根据预算和时间节点剔除不合理的路径。
# GoT (Graph of Thoughts) - 思维图
核心逻辑：非线性网络与知识聚合
GoT 是对 ToT 的进一步升华，它认为人类的思维不只是分叉的树，还可以是错综复杂的有向图（DAG）。是 **Agent 的设计范式**

- 工作方式：
    - 节点与边：每个思维是一个节点，边表示推理关系。
    - 变换操作：
        - 聚合（Aggregation）：将多个不同的思维节点合并成一个更完善的结论。
        - 精炼（Refinement）：对某个思维节点进行迭代修正。
        - 循环（Looping）：虽然通常是无环图，但在某些设计中允许对同一思维进行循环打磨。
- 核心优势：它可以处理具有复杂依赖关系的问题，支持不同推理路径之间的信息交换和优势互补。
## 应用场景
- 长文档摘要与信息整合： 如果要总结一本 300 页的书，GoT 可以先分章节总结（生成节点），然后将相关的章节总结进行合并（Aggregation），最后通过对比和去重，精炼出（Refinement）全书的核心观点。
- 复杂系统架构设计： 在设计一个微服务架构时，模型可以先生成存储方案和通信方案，然后将这两个节点连接起来，观察它们是否兼容。如果不兼容，则返回修改其中一个节点，直到整张图逻辑自洽。
- 新药发现或材料科学： 研究人员可以利用 GoT 让模型组合不同的化学分子式。节点 A 是某种特性，节点 B 是另一种特性，通过图的合并操作，探索是否存在某种新的组合能同时满足两者的优势。

# 位置编码的必要性
如果不添加位置编码，注意力计算的时候，注意力权重不会随位置变化，不符合需求，需要的是注意力可以感知到位置的差异性。
# 绝对位置编码
## 训练式位置编码
初始化一个向量维度 * 向量长度的位置编码矩阵，与embedding相加。但是这种方式不具备长度外推性，因为过长的向量的位置编码没有经过训练。
## sinusoidal位置编码
transformer中的经典位置编码，公式为：
$$
\begin{cases} 
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10,000^{\frac{2i}{d_{model}}}}\right) \\
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10,000^{\frac{2i}{d_{model}}}}\right) 
\end{cases}
$$
pos：行索引，代表第几个位置（第几个词）
i:列索引，代表ebd内部的维度
**$i$ 决定了“尺子的刻度”**：
随着 $i$ 的增大，$10000^{\frac{2i}{d_{model}}}$ 这个分母会指数级变大。这意味着：
- 当 $i$ 很小时（低维），波长很短，函数值随 $pos$ 的变化非常剧烈（高频）。
- 当 $i$ 很大时（高维），波长很长，函数值随 $pos$ 的变化非常缓慢（低频）。
 **$pos$ 是在刻度上的“取值点”**：
它决定了在当前频率的波形上，我们走到了哪一步。

这种编码方式的特点包括：
- Sinusoidal 编码具有周期性，有一定外推性，但效果有限
- 远程衰减性质：相同的ebd，距离越近，内积越高，反之越低。
---
# RoPE编码
**RoPE位置编码通过将一个向量旋转某个角度，为其赋予位置信息**
## 动机
绝对位置编码的方式，无法显式感知两个变量之间的相对位置关系。希望能实现相对的位置编码。核心逻辑在于：输入绝对位置m，n，输出的点积结果仅依赖于相对位移m-n。这样，模型可以学习到距离感，而非记住句子中的死板位置。
## 动机的公式说明
假设函数 $f(q, m)$ 表示为词向量 $q$ 添加绝对位置信息 $m$，得到带位置信息的向量 $q_m$：

$$q_m = f(q, m)$$
RoPE 希望 Query 向量 $q_m$ 与 Key 向量 $k_n$ 之间的**点积**能够包含相对位置信息 $m - n$。
为了实现这一目标，我们需要找到一个函数 $f$，使得两个位置向量的点积可以表示为一个仅关于词向量 $q, k$ 及其相对距离 $m - n$ 的函数 $g$：
$$f(q, m) \cdot f(k, n) = g(q, k, m - n)$$
## 实现方式
### 第一步：将向量看作复数 (2D 视角)

RoPE 首先将 $d$ 维的词向量两两一组，看作是 $\frac{d}{2}$ 个二维平面。对于其中一个二维平面上的向量 $[x_1, x_2]$，我们可以将其表示为一个复数 $z = x_1 + ix_2$。

### 第二步：应用旋转矩阵

为了引入位置信息 $m$，RoPE 不像传统方法那样做“加法”，而是做“乘法”——即将向量旋转一个角度。旋转角度与位置 $m$ 成正比：

$$f(q, m) = q \cdot e^{im\theta}$$

在二维矩阵的形式下，这等同于乘以一个**旋转矩阵**：

$$\begin{pmatrix} q_m^{(1)} \\ q_m^{(2)} \end{pmatrix} = \begin{pmatrix} \cos m\theta & -\sin m\theta \\ \sin m\theta & \cos m\theta \end{pmatrix} \begin{pmatrix} q^{(1)} \\ q^{(2)} \end{pmatrix}$$

其中，$\theta$ 是一个预设的频率常数
$$\theta_j = 10000^{-\frac{2(j-1)}{d}}$$
# 这里的 j = 1 代表向量的前两个维度 (index 0, 1)
# 这里的 j = 2 代表向量的下两个维度 (index 2, 3)

### 第三步：为什么点积满足相对位置？

这是 RoPE 最优雅的地方。根据复数的性质，两个复数点积（内积）等于一个复数与另一个复数的共轭相乘的实部。

如果我们有两个带位置信息的向量 $q_m$ 和 $k_n$：

1. $q_m$ 旋转了 $m\theta$ 度。
    
2. $k_n$ 旋转了 $n\theta$ 度。
    
3. 它们的点积结果会包含一项 $\cos(m\theta - n\theta)$，即 $\cos((m-n)\theta)$。
    

**结论：** 最终的点积结果只取决于它们的相对距离 $m-n$，完美达成了图片中所述的建模目标。
### 第四步：高维空间的整体实现

对于 $d$ 维向量，我们将整个向量切分成 $\frac{d}{2}$ 个部分，每一部分都按照上述逻辑旋转不同的角度 $\theta_i$：


$$
\mathbf{R}_{\Theta, m}^d \mathbf{x} = 
\begin{pmatrix}
\cos m\theta_1 & -\sin m\theta_1 & 0 & 0 & \cdots \\
\sin m\theta_1 & \cos m\theta_1 & 0 & 0 & \cdots \\
0 & 0 & \cos m\theta_2 & -\sin m\theta_2 & \cdots \\
0 & 0 & \sin m\theta_2 & \cos m\theta_2 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots
\end{pmatrix}
\begin{pmatrix}
x_1 \\ x_2 \\ x_3 \\ x_4 \\ \vdots
\end{pmatrix}
$$


---

### 总结 RoPE 的优势

- **外推性**：由于它是旋转，即使遇到比训练时更长的序列，模型也能通过旋转角度的线性增加来处理。
    
- **衰减性**：随着相对距离 $m-n$ 变大，点积强度会有自然衰减的趋势（长程衰减），符合直觉。
![[Pasted image 20260215122826.png]]
### 补充

- **外推性的局限**：Sinusoidal 编码虽然具有周期性，但在处理超出训练长度的序列时，模型往往难以泛化，效果远逊于 RoPE。
- **加法 vs 乘法**：
    - Sinusoidal 是 **Additive (加法)**：$Embedding = Word + Pos$。
    - RoPE 是 **Multiplicative (乘法/旋转)**：$Embedding = Rotate(Word, pos)$。这种旋转操作天然保持了向量的模长，仅改变相位，对优化更加友好。
- **Base 放大效应**：在长文本扩展中（如 LongLoRA），将 `base` 从 10,000 提升到 1,000,000 会拉长每个维度的周期。虽然增加了容纳长度，但也让“空间分辨率”变低了，模型会变得“近视”，分不清邻近位置的微小差异。因为：`base` 变大意味着 $\theta$ 变小，旋转得更“慢”了。就像时钟的指针走得极慢，导致相距很远的 $m$ 和 $n$ 对应的角度差依然很小，点积的 $\cos(m-n)\theta$ 依然接近 1。这确实会破坏模型对距离的敏感度。